{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "#from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Define data root directory\n",
    "#data_dir = \"./data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>joint_effort[0]</th>\n",
       "      <th>joint_effort[1]</th>\n",
       "      <th>joint_effort[2]</th>\n",
       "      <th>joint_effort[3]</th>\n",
       "      <th>joint_effort[4]</th>\n",
       "      <th>joint_effort[5]</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.050352</td>\n",
       "      <td>0.036084</td>\n",
       "      <td>0.153670</td>\n",
       "      <td>-0.208722</td>\n",
       "      <td>-0.226335</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.040706</td>\n",
       "      <td>0.052485</td>\n",
       "      <td>0.160204</td>\n",
       "      <td>-0.204533</td>\n",
       "      <td>-0.222889</td>\n",
       "      <td>0.070273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.070528</td>\n",
       "      <td>0.033901</td>\n",
       "      <td>0.198342</td>\n",
       "      <td>-0.208156</td>\n",
       "      <td>-0.228432</td>\n",
       "      <td>0.070408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.078726</td>\n",
       "      <td>0.032952</td>\n",
       "      <td>0.203791</td>\n",
       "      <td>-0.208763</td>\n",
       "      <td>-0.228608</td>\n",
       "      <td>0.071877</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.046257</td>\n",
       "      <td>0.048101</td>\n",
       "      <td>0.215115</td>\n",
       "      <td>-0.205263</td>\n",
       "      <td>-0.223011</td>\n",
       "      <td>0.071184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   samples  joint_effort[0]  joint_effort[1]  joint_effort[2]  \\\n",
       "0        0         1.050352         0.036084         0.153670   \n",
       "1        1         1.040706         0.052485         0.160204   \n",
       "2        2         1.070528         0.033901         0.198342   \n",
       "3        3         1.078726         0.032952         0.203791   \n",
       "4        4         1.046257         0.048101         0.215115   \n",
       "\n",
       "   joint_effort[3]  joint_effort[4]  joint_effort[5]  cluster  \n",
       "0        -0.208722        -0.226335         0.071900        0  \n",
       "1        -0.204533        -0.222889         0.070273        0  \n",
       "2        -0.208156        -0.228432         0.070408        0  \n",
       "3        -0.208763        -0.228608         0.071877        0  \n",
       "4        -0.205263        -0.223011         0.071184        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The scaler objects will be stored in this dictionary so that our output test data from the model can be re-scaled during evaluation\n",
    "\n",
    "    \n",
    "# Store csv file in a Pandas DataFrame\n",
    "df = pd.read_csv('combined_labled_k6.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "35833    0\n",
       "35834    0\n",
       "35835    0\n",
       "35836    0\n",
       "35837    0\n",
       "Name: cluster, Length: 35838, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "sc = MinMaxScaler()\n",
    "label_sc = MinMaxScaler()\n",
    "#data = sc.fit_transform(df.values)\n",
    "# Obtaining the Scale for the labels(usage data) so that output can be re-scaled to actual value during evaluation\n",
    "label_sc.fit(df.cluster.values.reshape(-1,1))\n",
    "label_scalers = label_sc\n",
    "print(label_scalers)\n",
    "test_x = np.array(df.iloc[1000:1500,1:7])\n",
    "test_y = np.array(df.iloc[1000:1500,7:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from locale import currency\n",
    "#from pickletools import float8\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.window=5\n",
    "    def __len__(self):\n",
    "       \n",
    "        Q = len(self.df)\n",
    "        self.length=Q-self.window+1\n",
    "        return self.length\n",
    "\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        x=np.array(self.df)\n",
    "        if idx>= 35830:\n",
    "            idx=idx-self.window\n",
    "            X = x[idx:idx+self.window,1:7]\n",
    "            Y=  x[idx,7:8]\n",
    "        else:\n",
    "            X = x[idx:idx+self.window,1:7]\n",
    "            Y=  x[idx+self.window,7:8]\n",
    "        \n",
    "        return X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 5, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10163/2797386379.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  each_x = torch.tensor(each_x,dtype=float)\n",
      "/tmp/ipykernel_10163/2797386379.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  each_y = torch.tensor(each_y,dtype=float)\n"
     ]
    }
   ],
   "source": [
    "cs= CustomDataset(df)\n",
    "\n",
    "train_l=round(int(cs.__len__())*0.8)\n",
    "#print(train_l)\n",
    "batch_size=25\n",
    "val_l=int(cs.__len__())-train_l\n",
    "train_set, val_set = torch.utils.data.random_split(cs, [train_l, val_l])\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "for each_x, each_y in train_loader:\n",
    "    print(each_x.shape)\n",
    "    break\n",
    "    \n",
    "    \n",
    "val_loader = DataLoader(val_set, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "for each_x, each_y in val_loader:\n",
    "    each_x = torch.tensor(each_x,dtype=float)\n",
    "    each_y = torch.tensor(each_y,dtype=float)\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35838"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        #print(x,h)\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.lstm(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "\n",
    "\n",
    "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=5, model_type=\"GRU\"):\n",
    "    \n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]\n",
    "    output_dim = 1\n",
    "    n_layers = 2\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.time()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            if counter%200 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.time()\n",
    "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
    "        print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    return model\n",
    "\n",
    "def evaluate(model, test_x, test_y, label_scalers):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.time()\n",
    "    for i in test_y:\n",
    "        inp = torch.from_numpy(np.array(test_x[i]))\n",
    "        labs = torch.from_numpy(np.array(test_y[i]))\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "    print(\"Evaluation Time: {}\".format(str(time.time()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35834"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "len(val_set.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Epoch 1......Step: 200/1146....... Average Loss for Epoch: 0.2541107908450067\n",
      "Epoch 1......Step: 400/1146....... Average Loss for Epoch: 0.2389455007435754\n",
      "Epoch 1......Step: 600/1146....... Average Loss for Epoch: 0.22386216358902553\n",
      "Epoch 1......Step: 800/1146....... Average Loss for Epoch: 0.20559164744598094\n",
      "Epoch 1......Step: 1000/1146....... Average Loss for Epoch: 0.193593687061104\n",
      "Epoch 1/5 Done, Total Loss: 0.18250919365879542\n",
      "Total Time Elapsed: 35.41452932357788 seconds\n",
      "Epoch 2......Step: 200/1146....... Average Loss for Epoch: 0.10780633241403847\n",
      "Epoch 2......Step: 400/1146....... Average Loss for Epoch: 0.10042820368689717\n",
      "Epoch 2......Step: 600/1146....... Average Loss for Epoch: 0.09824502304332175\n",
      "Epoch 2......Step: 800/1146....... Average Loss for Epoch: 0.09560663290700176\n",
      "Epoch 2......Step: 1000/1146....... Average Loss for Epoch: 0.0956929579161806\n",
      "Epoch 2/5 Done, Total Loss: 0.0942174378329649\n",
      "Total Time Elapsed: 26.89289355278015 seconds\n",
      "Epoch 3......Step: 200/1146....... Average Loss for Epoch: 0.08290947761328425\n",
      "Epoch 3......Step: 400/1146....... Average Loss for Epoch: 0.07889989715040428\n",
      "Epoch 3......Step: 600/1146....... Average Loss for Epoch: 0.08217028739474093\n",
      "Epoch 3......Step: 800/1146....... Average Loss for Epoch: 0.08261956904374529\n",
      "Epoch 3......Step: 1000/1146....... Average Loss for Epoch: 0.08045449330192059\n",
      "Epoch 3/5 Done, Total Loss: 0.0781566813210453\n",
      "Total Time Elapsed: 20.96629500389099 seconds\n",
      "Epoch 4......Step: 200/1146....... Average Loss for Epoch: 0.07807487338315695\n",
      "Epoch 4......Step: 400/1146....... Average Loss for Epoch: 0.07563852884602966\n",
      "Epoch 4......Step: 600/1146....... Average Loss for Epoch: 0.073896967851324\n",
      "Epoch 4......Step: 800/1146....... Average Loss for Epoch: 0.07371422727796016\n",
      "Epoch 4......Step: 1000/1146....... Average Loss for Epoch: 0.07250367544277106\n",
      "Epoch 4/5 Done, Total Loss: 0.07302052694050608\n",
      "Total Time Elapsed: 20.745580434799194 seconds\n",
      "Epoch 5......Step: 200/1146....... Average Loss for Epoch: 0.07105412460514345\n",
      "Epoch 5......Step: 400/1146....... Average Loss for Epoch: 0.07247476273551001\n",
      "Epoch 5......Step: 600/1146....... Average Loss for Epoch: 0.06943474901694571\n",
      "Epoch 5......Step: 800/1146....... Average Loss for Epoch: 0.06824244884039217\n",
      "Epoch 5......Step: 1000/1146....... Average Loss for Epoch: 0.06754719411680708\n",
      "Epoch 5/5 Done, Total Loss: 0.06939234039007709\n",
      "Total Time Elapsed: 32.43396043777466 seconds\n",
      "Total Training Time: 136.45325875282288 seconds\n",
      "Starting Training of LSTM model\n",
      "Epoch 1......Step: 200/1146....... Average Loss for Epoch: 0.2520019207848236\n",
      "Epoch 1......Step: 400/1146....... Average Loss for Epoch: 0.24619968653307298\n",
      "Epoch 1......Step: 600/1146....... Average Loss for Epoch: 0.24398287141462788\n",
      "Epoch 1......Step: 800/1146....... Average Loss for Epoch: 0.2438333545421483\n",
      "Epoch 1......Step: 1000/1146....... Average Loss for Epoch: 0.237531598221045\n",
      "Epoch 1/5 Done, Total Loss: 0.23000256110301295\n",
      "Total Time Elapsed: 35.215126752853394 seconds\n",
      "Epoch 2......Step: 200/1146....... Average Loss for Epoch: 0.15225761680398137\n",
      "Epoch 2......Step: 400/1146....... Average Loss for Epoch: 0.13678359785524663\n",
      "Epoch 2......Step: 600/1146....... Average Loss for Epoch: 0.12929959861328824\n",
      "Epoch 2......Step: 800/1146....... Average Loss for Epoch: 0.12709012595703825\n",
      "Epoch 2......Step: 1000/1146....... Average Loss for Epoch: 0.12275455288169905\n",
      "Epoch 2/5 Done, Total Loss: 0.1200947186792388\n",
      "Total Time Elapsed: 25.125800371170044 seconds\n",
      "Epoch 3......Step: 200/1146....... Average Loss for Epoch: 0.10183890874497592\n",
      "Epoch 3......Step: 400/1146....... Average Loss for Epoch: 0.09704297664007754\n",
      "Epoch 3......Step: 600/1146....... Average Loss for Epoch: 0.09219033553100113\n",
      "Epoch 3......Step: 800/1146....... Average Loss for Epoch: 0.09194208999986586\n",
      "Epoch 3......Step: 1000/1146....... Average Loss for Epoch: 0.08886280829430325\n",
      "Epoch 3/5 Done, Total Loss: 0.08714422852853938\n",
      "Total Time Elapsed: 20.960516929626465 seconds\n",
      "Epoch 4......Step: 200/1146....... Average Loss for Epoch: 0.07746836884791264\n",
      "Epoch 4......Step: 400/1146....... Average Loss for Epoch: 0.0744372074144485\n",
      "Epoch 4......Step: 600/1146....... Average Loss for Epoch: 0.07485617741704724\n",
      "Epoch 4......Step: 800/1146....... Average Loss for Epoch: 0.07568050193727686\n",
      "Epoch 4......Step: 1000/1146....... Average Loss for Epoch: 0.0753207864605647\n",
      "Epoch 4/5 Done, Total Loss: 0.07584243643404465\n",
      "Total Time Elapsed: 21.47732138633728 seconds\n",
      "Epoch 5......Step: 200/1146....... Average Loss for Epoch: 0.07241698738042032\n",
      "Epoch 5......Step: 400/1146....... Average Loss for Epoch: 0.06982400112014148\n",
      "Epoch 5......Step: 600/1146....... Average Loss for Epoch: 0.07220095885034729\n",
      "Epoch 5......Step: 800/1146....... Average Loss for Epoch: 0.0698293347514118\n",
      "Epoch 5......Step: 1000/1146....... Average Loss for Epoch: 0.06845999729481991\n",
      "Epoch 5/5 Done, Total Loss: 0.0678750498793868\n",
      "Total Time Elapsed: 21.42757296562195 seconds\n",
      "Total Training Time: 124.20633840560913 seconds\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "gru_model = train(train_loader, lr, model_type=\"GRU\")\n",
    "Lstm_model = train(train_loader, lr, model_type=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx should also be 2-D but got 3-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gru_outputs, targets, gru_sMAPE \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgru_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_scalers\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [95], line 61\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, test_x, test_y, label_scalers)\u001b[0m\n\u001b[1;32m     59\u001b[0m labs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(test_y[i]))\n\u001b[1;32m     60\u001b[0m h \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit_hidden(inp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 61\u001b[0m out, h \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(label_scalers[i]\u001b[38;5;241m.\u001b[39minverse_transform(out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     63\u001b[0m targets\u001b[38;5;241m.\u001b[39mappend(label_scalers[i]\u001b[38;5;241m.\u001b[39minverse_transform(labs\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [84], line 13\u001b[0m, in \u001b[0;36mGRUNet.forward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, h):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#print(x,h)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     out, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, h\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:927\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[39mif\u001b[39;00m hx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    926\u001b[0m         \u001b[39mif\u001b[39;00m hx\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 927\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    928\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFor unbatched 2-D input, hx should also be 2-D but got \u001b[39m\u001b[39m{\u001b[39;00mhx\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    929\u001b[0m         hx \u001b[39m=\u001b[39m hx\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    930\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx should also be 2-D but got 3-D tensor"
     ]
    }
   ],
   "source": [
    "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
