{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>joint_effort[0]</th>\n",
       "      <th>joint_effort[1]</th>\n",
       "      <th>joint_effort[2]</th>\n",
       "      <th>joint_effort[3]</th>\n",
       "      <th>joint_effort[4]</th>\n",
       "      <th>joint_effort[5]</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.050352</td>\n",
       "      <td>0.036084</td>\n",
       "      <td>0.153670</td>\n",
       "      <td>-0.208722</td>\n",
       "      <td>-0.226335</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.040706</td>\n",
       "      <td>0.052485</td>\n",
       "      <td>0.160204</td>\n",
       "      <td>-0.204533</td>\n",
       "      <td>-0.222889</td>\n",
       "      <td>0.070273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.070528</td>\n",
       "      <td>0.033901</td>\n",
       "      <td>0.198342</td>\n",
       "      <td>-0.208156</td>\n",
       "      <td>-0.228432</td>\n",
       "      <td>0.070408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.078726</td>\n",
       "      <td>0.032952</td>\n",
       "      <td>0.203791</td>\n",
       "      <td>-0.208763</td>\n",
       "      <td>-0.228608</td>\n",
       "      <td>0.071877</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.046257</td>\n",
       "      <td>0.048101</td>\n",
       "      <td>0.215115</td>\n",
       "      <td>-0.205263</td>\n",
       "      <td>-0.223011</td>\n",
       "      <td>0.071184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   samples  joint_effort[0]  joint_effort[1]  joint_effort[2]  \\\n",
       "0        0         1.050352         0.036084         0.153670   \n",
       "1        1         1.040706         0.052485         0.160204   \n",
       "2        2         1.070528         0.033901         0.198342   \n",
       "3        3         1.078726         0.032952         0.203791   \n",
       "4        4         1.046257         0.048101         0.215115   \n",
       "\n",
       "   joint_effort[3]  joint_effort[4]  joint_effort[5]  cluster  \n",
       "0        -0.208722        -0.226335         0.071900        0  \n",
       "1        -0.204533        -0.222889         0.070273        0  \n",
       "2        -0.208156        -0.228432         0.070408        0  \n",
       "3        -0.208763        -0.228608         0.071877        0  \n",
       "4        -0.205263        -0.223011         0.071184        0  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store csv file in a Pandas DataFrame\n",
    "df = pd.read_csv('combined_labled_k6.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f54c90c9dc0>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'cluster', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joint_effort[0]</th>\n",
       "      <th>joint_effort[1]</th>\n",
       "      <th>joint_effort[2]</th>\n",
       "      <th>joint_effort[3]</th>\n",
       "      <th>joint_effort[4]</th>\n",
       "      <th>joint_effort[5]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.050352</td>\n",
       "      <td>0.036084</td>\n",
       "      <td>0.153670</td>\n",
       "      <td>-0.208722</td>\n",
       "      <td>-0.226335</td>\n",
       "      <td>0.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.040706</td>\n",
       "      <td>0.052485</td>\n",
       "      <td>0.160204</td>\n",
       "      <td>-0.204533</td>\n",
       "      <td>-0.222889</td>\n",
       "      <td>0.070273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.070528</td>\n",
       "      <td>0.033901</td>\n",
       "      <td>0.198342</td>\n",
       "      <td>-0.208156</td>\n",
       "      <td>-0.228432</td>\n",
       "      <td>0.070408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.078726</td>\n",
       "      <td>0.032952</td>\n",
       "      <td>0.203791</td>\n",
       "      <td>-0.208763</td>\n",
       "      <td>-0.228608</td>\n",
       "      <td>0.071877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.046257</td>\n",
       "      <td>0.048101</td>\n",
       "      <td>0.215115</td>\n",
       "      <td>-0.205263</td>\n",
       "      <td>-0.223011</td>\n",
       "      <td>0.071184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35833</th>\n",
       "      <td>2.197765</td>\n",
       "      <td>0.698508</td>\n",
       "      <td>-0.035440</td>\n",
       "      <td>-0.127779</td>\n",
       "      <td>-0.357269</td>\n",
       "      <td>-0.376626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35834</th>\n",
       "      <td>2.181485</td>\n",
       "      <td>0.686810</td>\n",
       "      <td>-0.030687</td>\n",
       "      <td>-0.127087</td>\n",
       "      <td>-0.355794</td>\n",
       "      <td>-0.378369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35835</th>\n",
       "      <td>2.187751</td>\n",
       "      <td>0.723994</td>\n",
       "      <td>-0.046943</td>\n",
       "      <td>-0.124015</td>\n",
       "      <td>-0.355552</td>\n",
       "      <td>-0.374260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35836</th>\n",
       "      <td>2.173231</td>\n",
       "      <td>0.764835</td>\n",
       "      <td>-0.166300</td>\n",
       "      <td>-0.116387</td>\n",
       "      <td>-0.359594</td>\n",
       "      <td>-0.378620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35837</th>\n",
       "      <td>2.213714</td>\n",
       "      <td>0.708890</td>\n",
       "      <td>-0.019681</td>\n",
       "      <td>-0.124139</td>\n",
       "      <td>-0.360913</td>\n",
       "      <td>-0.377327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35838 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       joint_effort[0]  joint_effort[1]  joint_effort[2]  joint_effort[3]  \\\n",
       "0             1.050352         0.036084         0.153670        -0.208722   \n",
       "1             1.040706         0.052485         0.160204        -0.204533   \n",
       "2             1.070528         0.033901         0.198342        -0.208156   \n",
       "3             1.078726         0.032952         0.203791        -0.208763   \n",
       "4             1.046257         0.048101         0.215115        -0.205263   \n",
       "...                ...              ...              ...              ...   \n",
       "35833         2.197765         0.698508        -0.035440        -0.127779   \n",
       "35834         2.181485         0.686810        -0.030687        -0.127087   \n",
       "35835         2.187751         0.723994        -0.046943        -0.124015   \n",
       "35836         2.173231         0.764835        -0.166300        -0.116387   \n",
       "35837         2.213714         0.708890        -0.019681        -0.124139   \n",
       "\n",
       "       joint_effort[4]  joint_effort[5]  \n",
       "0            -0.226335         0.071900  \n",
       "1            -0.222889         0.070273  \n",
       "2            -0.228432         0.070408  \n",
       "3            -0.228608         0.071877  \n",
       "4            -0.223011         0.071184  \n",
       "...                ...              ...  \n",
       "35833        -0.357269        -0.376626  \n",
       "35834        -0.355794        -0.378369  \n",
       "35835        -0.355552        -0.374260  \n",
       "35836        -0.359594        -0.378620  \n",
       "35837        -0.360913        -0.377327  \n",
       "\n",
       "[35838 rows x 6 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "35833    0\n",
       "35834    0\n",
       "35835    0\n",
       "35836    0\n",
       "35837    0\n",
       "Name: cluster, Length: 35838, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train dataclass TrainData(Dataset):\n",
    "#from http.client import _DataType\n",
    "\n",
    "\n",
    "class TrainData(Dataset):   \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.window=5\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if index>= self.length-self.window:\n",
    "            self.X_data[index:index+self.window]\n",
    "            self.y_data[index]\n",
    "        else:\n",
    "            self.X_data[index:index+self.window]  \n",
    "            self.y_data[index+self.window]\n",
    "        \n",
    "        return self.X_data, self.y_data\n",
    "        \n",
    "    def __len__ (self):\n",
    "        Q = len(self.X_data)\n",
    "        self.length=Q-self.window+1\n",
    "        return self.length\n",
    "\n",
    "\n",
    "train_data = TrainData(torch.FloatTensor(X_train), \n",
    "                       torch.IntTensor(y_train))\n",
    "\n",
    "## test data  \n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data,y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.window=5\n",
    "    def __getitem__(self, index):\n",
    "        if index>= self.length-self.window:\n",
    "            self.X_data[index:index+self.window]\n",
    "            self.y_data[index]\n",
    "\n",
    "        else:\n",
    "            self.X_data[index:index+self.window]  \n",
    "            self.y_data[index+self.window]\n",
    "        \n",
    "        return self.X_data, self.y_data\n",
    "    def __len__ (self):\n",
    "        Q = len(self.X_data)\n",
    "        self.length=Q-self.window+1\n",
    "        return self.length\n",
    "    \n",
    "\n",
    "test_data = TestData(torch.FloatTensor(X_test),torch.IntTensor(y_train))                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "torch.Size([25, 28670, 6])\n",
      "torch.Size([25, 28670])\n",
      "torch.Size([25, 7168, 6])\n",
      "torch.Size([25, 28670])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "batch_size=25\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "\n",
    "val_loader = DataLoader(dataset=test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "print(next(iter(train_loader))[0].shape[0])\n",
    "#print(train_loader.batch_size)\n",
    "for each_x, each_y in train_loader:\n",
    "    print(each_x.shape)\n",
    "    print(each_y.shape)\n",
    "    break\n",
    "    \n",
    "    \n",
    "\n",
    "for each_x,each_y in val_loader:\n",
    "    print(each_x.shape)\n",
    "    print(each_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        #print(x,h)\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.lstm(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=5, model_type=\"GRU\"):\n",
    "    #print(next(iter(train_loader))[0].shape[1])\n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]\n",
    "    output_dim = 1\n",
    "    n_layers = 2\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.time()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            if counter%200 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.time()\n",
    "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
    "        print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_x, test_y, label_scalers):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.time()\n",
    "    j=0\n",
    "    for i in test_x:\n",
    "        inp = torch.from_numpy(np.array(test_x[j]))\n",
    "        labs = torch.from_numpy(np.array(test_y[j]))\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
    "        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
    "        j+=1\n",
    "    print(\"Evaluation Time: {}\".format(str(time.time()-start_time)))\n",
    "    sMAPE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
    "    return outputs, targets, sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 700.00 MiB (GPU 0; 3.82 GiB total capacity; 237.05 MiB already allocated; 366.69 MiB free; 906.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [190], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m----> 2\u001b[0m gru_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGRU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [188], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, learn_rate, hidden_dim, EPOCHS, model_type)\u001b[0m\n\u001b[1;32m     32\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([e\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m h])\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 35\u001b[0m out, h \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, label\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     37\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [187], line 13\u001b[0m, in \u001b[0;36mGRUNet.forward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, h):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#print(x,h)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     out, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, h\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:950\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    949\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 950\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    951\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    952\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    954\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 700.00 MiB (GPU 0; 3.82 GiB total capacity; 237.05 MiB already allocated; 366.69 MiB free; 906.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "gru_model = train(train_loader, lr, model_type=\"GRU\")\n",
    "#Lstm_model = train(train_loader, lr, model_type=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gru_outputs, targets, gru_sMAPE = evaluate(gru_model, X_test, y_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
